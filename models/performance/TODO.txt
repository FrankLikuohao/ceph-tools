generic:
	test.py
		get random/seq from dictionary
	fstest.py
		add main()
	filestoretest.py
		add main()
	radostest.py
		add main()

	DataFS.py
	    see if filestore atop DataFS returns same results
		4K, 128K reads are 40% fast
		4M writes are half speed (could this be an interpolation error)?

	    can I use it to simulate a disk as well

	simCPU ... thread/process/dma calibration

	SimNIC ... send/rcv memory multiplier for TCP/IP
		   send/rcv bus multiplier for TCP/IP
		   tot(bytes) = minCPU + memx*mem(bytes) + prox * proc(bytes)

	get validation data
		xfs file system throughput
		btrfs file system throughput
			SimFS simulation has changed enough to have completely broken the btrfs
			calibration.  Use new numbers to recalibrate.
		ext4 file system throughput
		zfs file system throughput
		SimIFC to get real multipliers


ceph:
	FileStore.py
	    mutatis mutandum
		exploit HBA simulation
		exploit NIC simulation
		new bandwidth limitation/warning model
		move to returning (latency, bw, usage)

	    validation plan
		I am not sure how much I trust the caching model for how much I/O have to do.
		I need to run this model against many more machines.
		use datafs to validate filestore results

	    new features
		discuss erasure coding with Sam
		exploit CPU simulation
		figure out how to better do create/delete
		figure out how to simulate cache loading

	Rados.py
	    mutatis mutandum
		exploit NIC simulation
		new bandwidth limitation/warning model
		move to returning (latency, bw, usage)

	    validation plan
		use datafs to validate rados results

	    new features
		discuss erasure coding with Sam
		figure out how to simulate cache loading


INVESTIGATE
    Rados:
	The write numbers are about 30% higher than the multiples of FileStore.
	The read numbers are 3x the multiples of FileStore.
	It seems clear that caching has gone insane.

IN MY DREAMS:
    more rational performance parameters for SimDisk.py
 
